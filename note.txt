关于交叉熵：
    1.是信息量的期望值，是一个随机变量的确定性的度量。
    2.熵越大，变量的取值就越不稳定，反之就越确定。
    3.softmax_cross_entropy_with_logits的原理是：
        （1）最后输出层做softmax，一个样本的结果就是一个n-class的归一化向量；
        （2）再和实际样本的每个分类值做一个交叉熵运算（一般情况下实际值只有一个为1，其余为0）


关于深度学习的认识：
    1.目前深度学习所解决的问题，归根结底，是回归和分类两大问题。
    2.回归用来预测，分类用来识别，所以在用深度学习解决问题时，先判断当前问题属于回归还是分类。
    3.做回归时，损失函数一般为均方损失函数；分类时，损失函数一般是softmax基础上的交叉熵损失函数。
    4.回归模型主要为循环神经网络RNN，如LSTM、bidirectional RNN、Deep RNN等。
    5.分类模型主要为卷积神经网络CNN，如VGGNet、Interception、Faster-RCNN、SSD、YOLO、FaceNet等。
